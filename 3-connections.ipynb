{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cloudy-silver",
   "metadata": {},
   "source": [
    "# Connecting to External Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-prison",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-mailman",
   "metadata": {},
   "source": [
    "Now so far, we have used airflow to setup a dag, and perform some tasks in Python. But as we know, the main usecase for airflow is performing our ETL operations.  This means that we'll need airflow to connect to external services like our RDS instance, our redshift cluster, and S3.  As we'll see in this lesson, airflow has some built in functionality that allows us to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-slope",
   "metadata": {},
   "source": [
    "### External Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-sharp",
   "metadata": {},
   "source": [
    "Now if we were to purely use Python, we know how to connect with postgres or RDS.  Here's how we would do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-kinase",
   "metadata": {},
   "source": [
    "```python\n",
    "import psycopg2\n",
    "HOST = 'host_url'\n",
    "conn = psycopg2.connect(host = HOST, \n",
    "                 user = 'postgres', \n",
    "                 password = 'postgres', \n",
    "                database = 'dev', port = '5432')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-dynamics",
   "metadata": {},
   "source": [
    "So we would connect to our database by creating a connection object with use of the psycopg2 library.  There, we specify the host url, whether it be localhost or that of our redshift or RDS server, and the related username, password and database information.\n",
    "\n",
    "In airflow, we do something similar, but we pass that information through the web interface.\n",
    "\n",
    "To find the panel to do so, we can bootup airflow, and then click on `Admin` followed by `Connections`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-comment",
   "metadata": {},
   "source": [
    "<img src=\"./connections_ux.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-confirmation",
   "metadata": {},
   "source": [
    "From there, we can click on create connection, and we'll find a form to enter in similar information that we use to connect to our database through psycopg2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-spirit",
   "metadata": {},
   "source": [
    "> Looking at the information below, the only thing new is the `Conn id`.  That's what we'll use to reference the connection to our RDS instance in our code.  We also specify a Conn Type of Postgres, as that's the technology that RDS is built on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-bernard",
   "metadata": {},
   "source": [
    "<img src=\"./rds_updated.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-silver",
   "metadata": {},
   "source": [
    "> **Warning**: There is one thing pretty confusing from the above.  Where the field says `Schema`, we actually enter in the database that we would like to connect to.  So above, the database we are connecting to is the `postgres` database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-coordination",
   "metadata": {},
   "source": [
    "So when we're done entering in our information, we can click `Save`.  We can now see our rds connection listed among the rest of the connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-president",
   "metadata": {},
   "source": [
    "<img src=\"./rds_link.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-lloyd",
   "metadata": {},
   "source": [
    "### Referencing our Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-helicopter",
   "metadata": {},
   "source": [
    "So we just entered in the information needed to connect to our RDS instance.  And we added an `Conn id` of `rds` so that we can reference that information in our code.  Let's see how we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-pocket",
   "metadata": {},
   "source": [
    "The key part is right here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-postage",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def retrieve_states():\n",
    "    rds_hook = PostgresHook('rds')\n",
    "    vals = rds_hook.get_records(\"\"\"SELECT * FROM states;\"\"\")\n",
    "    return vals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-kuwait",
   "metadata": {},
   "source": [
    "We define a function called `retreive_states` and then we use a `PostgresHook` object to make the connection to our rds database, by passing through the `Conn Id` we specified in the web interface.  From there we called the `get_records` function followed by our SQL call.  We then returned the retrieved values from the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-circle",
   "metadata": {},
   "source": [
    "Now, once we connect that function up to a task, and connect the task to a DAG, we are good to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-retirement",
   "metadata": {},
   "source": [
    "```python \n",
    "get_foursquare_info = DAG(dag_id = 'retrieve_foursquare_data', \n",
    "                          start_date = datetime.now() - timedelta(days = 1))\n",
    "\n",
    "\n",
    "get_states = PythonOperator(task_id='get_states', \n",
    "                            dag = get_foursquare_info,\n",
    "                            python_callable = retrieve_states)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-calgary",
   "metadata": {},
   "source": [
    "When we trigger the dag with that task, we can see that data is returned by looking at the logs of our `get_states` task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-partner",
   "metadata": {},
   "source": [
    "> Below we see the listed states of `New York` and `Pennsylvania`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-business",
   "metadata": {},
   "source": [
    "<img src=\"./log_get_states.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-latest",
   "metadata": {},
   "source": [
    "### Using Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-border",
   "metadata": {},
   "source": [
    "Now another way that we could perform connect to our RDS database is with an operator.  In addition to our Python operator, airflow also gives us a postgres operator.  So this time, we could perform the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-macintosh",
   "metadata": {},
   "source": [
    "```python\n",
    "select_states = PostgresOperator(\n",
    "        task_id=\"select_states\",\n",
    "        postgres_conn_id=\"rds\",\n",
    "        sql=\"\"\"SELECT * FROM states;\"\"\", \n",
    "        dag = get_foursquare_info\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-destruction",
   "metadata": {},
   "source": [
    "So here, we do not need to make our connection through the hook.  Instead, our task directly references the rds connection through the `postgres_conn_id` argument.  The downside to using our operator however, is that while it will execute the query, we will not see our query logged.  So using the operator is better for commands where we are making a change to our database or records -- like inserting records, or creating a new table.  In those cases, we would like to perform a command on our postgres database, but we are not querying for information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-egyptian",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-chassis",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to connect to our AWS services by using creating connections through the web interface.  We saw that through airflow's web interface we entered in the information for connecting to our RDS instance, including the `Conn Id`.  One confusing component was that where the form says `Schema`, we entered in the name of our database.  \n",
    "\n",
    "Then we were able to reference the connection to our database through a redshift hook, where we then used `get_records` to perform our query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-converter",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "def retrieve_states():\n",
    "    rds_hook = PostgresHook('rds')\n",
    "    vals = rds_hook.get_records(\"\"\"SELECT * FROM states;\"\"\")\n",
    "    return vals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-perfume",
   "metadata": {},
   "source": [
    "We also connected to the database, using our postgres operator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-colorado",
   "metadata": {},
   "source": [
    "```python\n",
    "select_states = PostgresOperator(\n",
    "        task_id=\"select_states\",\n",
    "        postgres_conn_id=\"rds\",\n",
    "        sql=\"\"\"SELECT * FROM states;\"\"\", \n",
    "        dag = get_foursquare_info\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
